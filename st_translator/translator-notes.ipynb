{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from functools import reduce\n",
    "import mafan\n",
    "from mafan import text\n",
    "import itertools\n",
    "bos = \" <bos> \"\n",
    "eos = \" <eos> \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer Functions\n",
    "\n",
    "## Sentence Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zng(paragraph):\n",
    "    for sent in re.findall(u'[^!?。\\.\\!\\?]+[!?。\\.\\!\\?]?', paragraph, flags=re.U):\n",
    "        yield sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplified Chinese Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code for simplified to traditional mapping dictionary.\n",
    "\n",
    "We have a large dictionary *conversions.txt* that includes words, characters, common phrases, locations and idioms. Each entry contains the traditional chinese word and simplified chinese word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"conversions.txt\", \"r+\", encoding=\"utf-8\")\n",
    "\n",
    "s2t_dict = dict()\n",
    "\n",
    "for line in infile:\n",
    "    line = line.rstrip()\n",
    "    arr = line.split()\n",
    "    trad = arr[0]\n",
    "    sim = arr[1]\n",
    "    if sim not in s2t_dict:\n",
    "        s2t_dict[sim] = [trad]\n",
    "    else:\n",
    "        s2t_dict[sim].append(trad)\n",
    "s2t_dict['-'] = ['-']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokeniser is used for identifying dictionary words and phrases in the input sentence. We always prefer longer phrases because it gives more meaning and less translation mappings. Hence we use Byte Pair Encoding (BPE) for identifying words, while BPE candidates are constrained by the defined list of vocabs in the dictionary. Since the longest phrase in the dictionary has 8 characters we start with 8-character phrases and do it backwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(sentence, n = 8):\n",
    "    '''\n",
    "    This function tokenizes input sentences according to the dicitionary.\n",
    "    Input: a sentence or paragraph\n",
    "    Output: a list of tokens from the input in order according to the original paragraph; a list of non-chinese characters from the original text.\n",
    "    '''\n",
    "    text, charList = prepare(sentence)\n",
    "    token_list = []\n",
    "    input_text = text\n",
    "    for k in range(n, 0, -1):\n",
    "        candidates = [input_text[i:i + k] for i in range(len(input_text) - k + 1)]\n",
    "        for candidate in candidates:\n",
    "            if candidate in s2t_dict:\n",
    "                token_list.append(candidate)\n",
    "                input_text = re.sub(candidate, '', input_text)\n",
    "    final = sequencer(token_list, text)\n",
    "    return final, charList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_list(sentence_list, char_list):\n",
    "    count = 0\n",
    "    original = [] # sentence we want to output\n",
    "    \n",
    "    for word in sentence_list:\n",
    "        if \"-\" in word:\n",
    "            original.append(list(char_list[count]))\n",
    "            count += 1\n",
    "        else:\n",
    "            original.append(word)\n",
    "    return original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output(sentence, char_list):\n",
    "    count = 0\n",
    "    original = \"\" # sentence we want to output\n",
    "\n",
    "    for char in list(sentence):\n",
    "        if char == \"-\":\n",
    "            original += char_list[count] # append character if non-chinese\n",
    "            count += 1\n",
    "        else:\n",
    "            original += char # append chinese\n",
    "    return original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(sentence):\n",
    "    new = \"\" # input to your tokenizer\n",
    "    char_list = [] # punct / english to be omitted\n",
    "\n",
    "    for char in list(sentence):\n",
    "        if text.identify(char) is mafan.NEITHER:\n",
    "            new += \"-\" # sub - with non-chinese chars\n",
    "            char_list.append(char)\n",
    "        else:\n",
    "            new += char\n",
    "\n",
    "    return new, char_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequencer(tokens, example):\n",
    "\n",
    "    flags = [1] * len(example)\n",
    "    sequence = []\n",
    "    for token in tokens:\n",
    "        for match in re.finditer(token, example):\n",
    "            location = (token, match.span()[0], match.span()[1])\n",
    "            valid = reduce(lambda x,y:x*y, flags[location[1]:location[2]])\n",
    "            if valid:\n",
    "                sequence.append(location)\n",
    "                for i in range(location[1], location[2]):\n",
    "                    flags[i] = 0\n",
    "            else:\n",
    "                continue\n",
    "    sequence.sort(key=lambda x: x[1])\n",
    "    result = [x[0] for x in sequence]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to prepare our corpus.\n",
    "1. We will add paddings (sentinels) to our sentences.\n",
    "2. Take one sentence at a time.\n",
    "3. Change non-chinese words to FW to avoid data explosion.\n",
    "4. Slice the n-grams and add them to dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_stuff(order):\n",
    "    '''\n",
    "    This function divides the corpus into n-grams and stores them in dictionary.\n",
    "    Input: order of n-gram (like 2 for bi-gram)\n",
    "    Output: none\n",
    "    '''\n",
    "    infile = open(\"hk-zh.txt\", \"r+\") # this contains our corpus\n",
    "    start_padding = bos * order # add padding\n",
    "    end_padding = eos * order\n",
    "\n",
    "    for line in tqdm(infile, total=1314726):\n",
    "        line = line.rstrip()\n",
    "        sentences = list(zng(line)) # tokenize sentence by sentence\n",
    "        for sentence in sentences:\n",
    "            candidate = start_padding + sentence + end_padding # form sentence\n",
    "            word_list = candidate.split()\n",
    "            word_list_tokens = []\n",
    "            for word in word_list:\n",
    "                if not(bool(re.match('^[a-zA-Z0-9]+$', word))):\n",
    "                    word_list_tokens.append(word) # add if not chinese\n",
    "                else:\n",
    "                    word_list_tokens.append(\"FW\") # turn non-chinese (except punc) to FW\n",
    "            word_list = word_list_tokens\n",
    "            ordered = [word_list[i:i + order] for i in range(1, len(word_list) - order)] # extract n-grams through slicing\n",
    "            # for each ngram, convert to tuple and add to dictionary\n",
    "            for ngram in ordered:\n",
    "                ngram = tuple(ngram)\n",
    "                if ngram not in corpus:\n",
    "                    corpus[ngram] = 1\n",
    "                else:\n",
    "                    corpus[ngram] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say you want to extract till trigrams.\n",
    "\n",
    "We want to do 3 iterations, for trigram, bi-gram and then unigram. Each iteration takes 2 minutes. This is only time-consuming part of this code. Once you prep the dictionary, you don't need to do this again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = dict()\n",
    "# start_order = 2\n",
    "# for i in range(start_order, 0, -1):\n",
    "#     add_stuff(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you made the dictionary, dump it into a pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# with open('corpus.pkl', 'wb') as handle:\n",
    "#     pickle.dump(corpus, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a way to load a pickle so you don't need to process data everytime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('corpus3.pkl', 'rb') as fp:\n",
    "    corpus = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Candidate Lists\n",
    "\n",
    "1. Tokenize the input.\n",
    "2. Check the mappings of each input.\n",
    "3. Add all possible mappings to candidate list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(sentence):\n",
    "    '''\n",
    "    Returns list of possible mappings.\n",
    "    Input: Simplified chinese sentence\n",
    "    Output: List of lists. Each list has a set of possible traditional chinese tokens\n",
    "    '''\n",
    "    tokens, char_list = tokenizer(sentence)\n",
    "    candidate_list = []\n",
    "    for token in tokens:\n",
    "        candidate_list.append(s2t_dict[token])\n",
    "    candidate_list = output_list(candidate_list, char_list)\n",
    "    return(candidate_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum log-likelihood calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = 4526000 # total number of tokens in corpus\n",
    "\n",
    "def prob(word_list):\n",
    "    '''\n",
    "    Computes the log likelihood probability.\n",
    "    Input: A sequence of words in form of list\n",
    "    Output: Log probabilties\n",
    "    '''\n",
    "    word_list = tuple(word_list) # change word list to tuple\n",
    "    if word_list in corpus:\n",
    "        # word found in dictionary\n",
    "        numerator = corpus[word_list] # get the frequency of that word list\n",
    "        denominator = num_tokens # let denominator be num tokens\n",
    "        # cutoff the last word and check whether it's in corpus\n",
    "        if len(word_list[:-1]) > 1 and word_list[:-1] in corpus:\n",
    "            denom_list = word_list[:-1]\n",
    "            denominator = corpus[denom_list]\n",
    "        return log(numerator / denominator) # log of prob\n",
    "    else:\n",
    "        word_list = list(word_list) # convert it back to list\n",
    "        k = len(word_list) - 1 # backoff, reduce n gram length\n",
    "        if k > 0:\n",
    "            # recursive function, divide the sequence into smaller n and find probs\n",
    "            probs = [prob(word_list[i:i + k]) for i in range(len(word_list) - k + 1)]\n",
    "            return sum(probs)\n",
    "        else:\n",
    "            # we found an unseen word\n",
    "            if not(bool(re.match('^[a-zA-Z0-9]+$', word_list[0]))):\n",
    "                return log(1 / num_tokens) # return a small probability\n",
    "            else:\n",
    "                return prob([\"FW\"]) # we encountered a non-chinese word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backoff Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "def backoff(sentence, order):\n",
    "    '''\n",
    "    Calcuates log likelihood using backoff language model\n",
    "    Input: Sentence and order of the n-gram\n",
    "    Output: Log prob of that sentence\n",
    "    '''\n",
    "    score = 0\n",
    "    sentences = list(zng(sentence)) # sentence tokenizer\n",
    "    for sentence in sentences:\n",
    "        start_padding = bos * order # beginning padding\n",
    "        end_padding = eos * order # ending padding\n",
    "        candidate = start_padding + sentence + end_padding # add paddings\n",
    "        word_list = candidate.split()\n",
    "        word_list_tokens = []\n",
    "        for word in word_list:\n",
    "            # append only non-chinese words\n",
    "            if not(bool(re.match('^[a-zA-Z0-9]+$', word))):\n",
    "                word_list_tokens.append(word)\n",
    "            else:\n",
    "                word_list_tokens.append(\"FW\")\n",
    "        word_list = word_list_tokens\n",
    "        ordered = [word_list[i:i + order] for i in range(1, len(word_list) - order)] # shingle into n-grams\n",
    "        probs = [prob(x) for x in ordered] # calculate probabilities\n",
    "        score += sum(probs) # final answer\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    '''\n",
    "    Translate a given sentence to traditional\n",
    "    Input: Simplified Sentence\n",
    "    Output: Traditional Sentence\n",
    "    '''\n",
    "    candidates = convert(sentence) # get the candidate lists\n",
    "    final_sent = \"\"\n",
    "    for words in candidates:\n",
    "        if len(words) > 1:\n",
    "            # many to one mappings\n",
    "            score = -50000.0 # start with extreme negative value\n",
    "            likely = \"\"\n",
    "            for candidate in words:\n",
    "                temp = final_sent\n",
    "                temp = temp + \" \"  + candidate # add a candidate to temp sentence\n",
    "                current_score = backoff(temp, 2) # check perplexity\n",
    "                if current_score > score:\n",
    "                    # if performing good, include that\n",
    "                    score = current_score\n",
    "                    likely = candidate\n",
    "            final_sent = final_sent + \" \" + likely\n",
    "        else:\n",
    "            final_sent = final_sent + \" \" + words[0]\n",
    "    final_sent = final_sent.replace(\" \", \"\")\n",
    "    final_sent = add_back_spaces(sentence, final_sent)\n",
    "    return final_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_back_spaces(original, current):\n",
    "    current_list = list(current)\n",
    "    original_list = list(original)\n",
    "    count = 1\n",
    "    for index, char in enumerate(original_list):\n",
    "        if char == \" \":\n",
    "            current_list[index - count] += \" \"\n",
    "            count += 1\n",
    "    current = \"\".join(current_list)\n",
    "    return current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "香港大學為亞洲之國際大學，藉國際化、創新性及跨範疇發揮其影響力；憑卓越研究、優秀教學、知識與技術之交流轉移，吸引及培育全球英才；並透過參與環球事務、其地區影響力及與中國內地之緊密連繫，為促進社會進步作出貢獻。\n"
     ]
    }
   ],
   "source": [
    "sent = \"香港大学为亚洲之国际大学，借国际化、创新性及跨范畴发挥其影响力；凭卓越研究、优秀教学、知识与技术之交流转移，吸引及培育全球英才；并透过参与环球事务、其地区影响力及与中国内地之紧密连系，为促进社会进步作出贡献。\"\n",
    "converted = translate(sent)\n",
    "print(converted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "自然語言處理是人工智能和語言學領域的分支學科。此領域探討如何處理及運用自然語言；自然語言處理包括多方面和步驟，基本有認知、理解、生成等部分\n"
     ]
    }
   ],
   "source": [
    "sent = \"自然语言处理是人工智能和语言学领域的分支学科。此领域探讨如何处理及运用自然语言；自然语言处理包括多方面和步骤，基本有认知、理解、生成等部分\"\n",
    "converted = translate(sent)\n",
    "print(converted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "早在23歲，伍茲就參與了世界上首個核反應堆Chicago Pile-1的建設，她是導師費米領導的項目團隊中最年輕的一員。此外，伍茲在建立和使用實驗所需的蓋革計數器上起到關鍵作用。反應堆成功運轉並達到自持狀態時，她也是唯一在場的女性。曼哈頓計劃中，她與費米合作；同時，她曾與第一任丈夫約翰·馬歇爾（John Marshall）一同解決了漢福德區鈈生產廠氙中毒的問題，並負責監督鈈生產反應爐的建造和運行。\n"
     ]
    }
   ],
   "source": [
    "sentence = \"早在23岁，伍兹就参与了世界上首个核反应堆Chicago Pile-1的建设，她是导师费米领导的项目团队中最年轻的一员。此外，伍兹在建立和使用实验所需的盖革计数器上起到关键作用。反应堆成功运转并达到自持状态时，她也是唯一在场的女性。曼哈顿计划中，她与费米合作；同时，她曾与第一任丈夫约翰·马歇尔（John Marshall）一同解决了汉福德区钚生产厂氙中毒的问题，并负责监督钚生产反应炉的建造和运行。\"\n",
    "converted = translate(sentence)\n",
    "print(converted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
