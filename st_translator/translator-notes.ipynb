{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from functools import reduce\n",
    "import mafan\n",
    "from mafan import text\n",
    "import itertools\n",
    "bos = \" <bos> \"\n",
    "eos = \" <eos> \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer Functions\n",
    "\n",
    "## Sentence Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zng(paragraph):\n",
    "    for sent in re.findall(u'[^!?。\\.\\!\\?]+[!?。\\.\\!\\?]?', paragraph, flags=re.U):\n",
    "        yield sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplified Chinese Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code for simplified to traditional mapping dictionary.\n",
    "\n",
    "We have a large dictionary *conversions.txt* that includes words, characters, common phrases, locations and idioms. Each entry contains the traditional chinese word and simplified chinese word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"conversions.txt\", \"r+\", encoding=\"utf-8\")\n",
    "\n",
    "s2t_dict = dict()\n",
    "\n",
    "for line in infile:\n",
    "    line = line.rstrip()\n",
    "    arr = line.split()\n",
    "    trad = arr[0]\n",
    "    sim = arr[1]\n",
    "    if sim not in s2t_dict:\n",
    "        s2t_dict[sim] = [trad]\n",
    "    else:\n",
    "        s2t_dict[sim].append(trad)\n",
    "s2t_dict['-'] = ['-']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokeniser is used for identifying dictionary words and phrases in the input sentence. We always prefer longer phrases because it gives more meaning and less translation mappings. Hence we use Byte Pair Encoding (BPE) for identifying words, while BPE candidates are constrained by the defined list of vocabs in the dictionary. Since the longest phrase in the dictionary has 8 characters we start with 8-character phrases and do it backwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(sentence, n = 8):\n",
    "    '''\n",
    "    This function tokenizes input sentences according to the dicitionary.\n",
    "    Input: a sentence or paragraph\n",
    "    Output: a list of tokens from the input in order according to the original paragraph; a list of non-chinese characters from the original text.\n",
    "    '''\n",
    "    text, charList = prepare(sentence)\n",
    "    token_list = []\n",
    "    input_text = text\n",
    "    for k in range(n, 0, -1):\n",
    "        candidates = [input_text[i:i + k] for i in range(len(input_text) - k + 1)]\n",
    "        for candidate in candidates:\n",
    "            if candidate in s2t_dict:\n",
    "                token_list.append(candidate)\n",
    "                input_text = re.sub(candidate, '', input_text)\n",
    "    final = sequencer(token_list, text)\n",
    "    return final, charList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_list(sentence_list, char_list):\n",
    "    count = 0\n",
    "    original = [] # sentence we want to output\n",
    "    \n",
    "    for word in sentence_list:\n",
    "        if \"-\" in word:\n",
    "            original.append(list(char_list[count]))\n",
    "            count += 1\n",
    "        else:\n",
    "            original.append(word)\n",
    "    return original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output(sentence, char_list):\n",
    "    count = 0\n",
    "    original = \"\" # sentence we want to output\n",
    "\n",
    "    for char in list(sentence):\n",
    "        if char == \"-\":\n",
    "            original += char_list[count] # append character if non-chinese\n",
    "            count += 1\n",
    "        else:\n",
    "            original += char # append chinese\n",
    "    return original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(sentence):\n",
    "    new = \"\" # input to your tokenizer\n",
    "    char_list = [] # punct / english to be omitted\n",
    "\n",
    "    for char in list(sentence):\n",
    "        if text.identify(char) is mafan.NEITHER:\n",
    "            new += \"-\" # sub - with non-chinese chars\n",
    "            char_list.append(char)\n",
    "        else:\n",
    "            new += char\n",
    "\n",
    "    return new, char_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequencer(tokens, example):\n",
    "\n",
    "    flags = [1] * len(example)\n",
    "    sequence = []\n",
    "    for token in tokens:\n",
    "        for match in re.finditer(token, example):\n",
    "            location = (token, match.span()[0], match.span()[1])\n",
    "            valid = reduce(lambda x,y:x*y, flags[location[1]:location[2]])\n",
    "            if valid:\n",
    "                sequence.append(location)\n",
    "                for i in range(location[1], location[2]):\n",
    "                    flags[i] = 0\n",
    "            else:\n",
    "                continue\n",
    "    sequence.sort(key=lambda x: x[1])\n",
    "    result = [x[0] for x in sequence]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to prepare our corpus.\n",
    "1. We will add paddings (sentinels) to our sentences.\n",
    "2. Take one sentence at a time.\n",
    "3. Change non-chinese words to FW to avoid data explosion.\n",
    "4. Slice the n-grams and add them to dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_stuff(order):\n",
    "    '''\n",
    "    This function divides the corpus into n-grams and stores them in dictionary.\n",
    "    Input: order of n-gram (like 2 for bi-gram)\n",
    "    Output: none\n",
    "    '''\n",
    "    infile = open(\"hk-zh.txt\", \"r+\") # this contains our corpus\n",
    "    start_padding = bos * order # add padding\n",
    "    end_padding = eos * order\n",
    "\n",
    "    for line in tqdm(infile, total=1314726):\n",
    "        line = line.rstrip()\n",
    "        sentences = list(zng(line)) # tokenize sentence by sentence\n",
    "        for sentence in sentences:\n",
    "            candidate = start_padding + sentence + end_padding # form sentence\n",
    "            word_list = candidate.split()\n",
    "            word_list_tokens = []\n",
    "            for word in word_list:\n",
    "                if not(bool(re.match('^[a-zA-Z0-9]+$', word))):\n",
    "                    word_list_tokens.append(word) # add if not chinese\n",
    "                else:\n",
    "                    word_list_tokens.append(\"FW\") # turn non-chinese (except punc) to FW\n",
    "            word_list = word_list_tokens\n",
    "            ordered = [word_list[i:i + order] for i in range(1, len(word_list) - order)] # extract n-grams through slicing\n",
    "            # for each ngram, convert to tuple and add to dictionary\n",
    "            for ngram in ordered:\n",
    "                ngram = tuple(ngram)\n",
    "                if ngram not in corpus:\n",
    "                    corpus[ngram] = 1\n",
    "                else:\n",
    "                    corpus[ngram] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say you want to extract till trigrams.\n",
    "\n",
    "We want to do 3 iterations, for trigram, bi-gram and then unigram. Each iteration takes 2 minutes. This is only time-consuming part of this code. Once you prep the dictionary, you don't need to do this again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = dict()\n",
    "# start_order = 2\n",
    "# for i in range(start_order, 0, -1):\n",
    "#     add_stuff(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you made the dictionary, dump it into a pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('corpus.pkl', 'wb') as handle:\n",
    "#     pickle.dump(corpus, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a way to load a pickle so you don't need to process data everytime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('corpus.pkl', 'rb') as fp:\n",
    "    corpus = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Candidate Lists\n",
    "\n",
    "1. Tokenize the input.\n",
    "2. Check the mappings of each input.\n",
    "3. Add all possible mappings to candidate list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(sentence):\n",
    "    '''\n",
    "    Returns list of possible mappings.\n",
    "    Input: Simplified chinese sentence\n",
    "    Output: List of lists. Each list has a set of possible traditional chinese tokens\n",
    "    '''\n",
    "    tokens, char_list = tokenizer(sentence)\n",
    "    candidate_list = []\n",
    "    for token in tokens:\n",
    "        candidate_list.append(s2t_dict[token])\n",
    "    candidate_list = output_list(candidate_list, char_list)\n",
    "    return(candidate_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum log-likelihood calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = 4526000 # total number of tokens in corpus\n",
    "\n",
    "def prob(word_list):\n",
    "    '''\n",
    "    Computes the log likelihood probability.\n",
    "    Input: A sequence of words in form of list\n",
    "    Output: Log probabilties\n",
    "    '''\n",
    "    word_list = tuple(word_list) # change word list to tuple\n",
    "    if word_list in corpus:\n",
    "        # word found in dictionary\n",
    "        numerator = corpus[word_list] # get the frequency of that word list\n",
    "        denominator = num_tokens # let denominator be num tokens\n",
    "        # cutoff the last word and check whether it's in corpus\n",
    "        if len(word_list[:-1]) > 1 and word_list[:-1] in corpus:\n",
    "            denom_list = word_list[:-1]\n",
    "            denominator = corpus[denom_list]\n",
    "        return log(numerator / denominator) # log of prob\n",
    "    else:\n",
    "        word_list = list(word_list) # convert it back to list\n",
    "        k = len(word_list) - 1 # backoff, reduce n gram length\n",
    "        if k > 0:\n",
    "            # recursive function, divide the sequence into smaller n and find probs\n",
    "            probs = [prob(word_list[i:i + k]) for i in range(len(word_list) - k + 1)]\n",
    "            return sum(probs)\n",
    "        else:\n",
    "            # we found an unseen word\n",
    "            if not(bool(re.match('^[a-zA-Z0-9]+$', word_list[0]))):\n",
    "                return log(1 / num_tokens) # return a small probability\n",
    "            else:\n",
    "                return prob([\"FW\"]) # we encountered a non-chinese word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backoff Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "def backoff(sentence, order):\n",
    "    '''\n",
    "    Calcuates log likelihood using backoff language model\n",
    "    Input: Sentence and order of the n-gram\n",
    "    Output: Log prob of that sentence\n",
    "    '''\n",
    "    score = 0\n",
    "    sentences = list(zng(sentence)) # sentence tokenizer\n",
    "    for sentence in sentences:\n",
    "        start_padding = bos * order # beginning padding\n",
    "        end_padding = eos * order # ending padding\n",
    "        candidate = start_padding + sentence + end_padding # add paddings\n",
    "        word_list = candidate.split()\n",
    "        word_list_tokens = []\n",
    "        for word in word_list:\n",
    "            # append only non-chinese words\n",
    "            if not(bool(re.match('^[a-zA-Z0-9]+$', word))):\n",
    "                word_list_tokens.append(word)\n",
    "            else:\n",
    "                word_list_tokens.append(\"FW\")\n",
    "        word_list = word_list_tokens\n",
    "        ordered = [word_list[i:i + order] for i in range(1, len(word_list) - order)] # shingle into n-grams\n",
    "        probs = [prob(x) for x in ordered] # calculate probabilities\n",
    "        score += sum(probs) # final answer\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    '''\n",
    "    Translate a given sentence to traditional\n",
    "    Input: Simplified Sentence\n",
    "    Output: Traditional Sentence\n",
    "    '''\n",
    "    candidates = convert(sentence) # get the candidate lists\n",
    "    final_sent = \"\"\n",
    "    for words in candidates:\n",
    "        if len(words) > 1:\n",
    "            # many to one mappings\n",
    "            score = -50000.0 # start with extreme negative value\n",
    "            likely = \"\"\n",
    "            for candidate in words:\n",
    "                temp = final_sent\n",
    "                temp = temp + \" \"  + candidate # add a candidate to temp sentence\n",
    "                current_score = backoff(temp, 2) # check perplexity\n",
    "                if current_score > score:\n",
    "                    # if performing good, include that\n",
    "                    score = current_score\n",
    "                    likely = candidate\n",
    "            final_sent = final_sent + \" \" + likely\n",
    "        else:\n",
    "            final_sent = final_sent + \" \" + words[0]\n",
    "    final_sent = final_sent.replace(\" \", \"\")\n",
    "    final_sent = add_back_spaces(sentence, final_sent)\n",
    "    return final_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_back_spaces(original, current):\n",
    "    current_list = list(current)\n",
    "    original_list = list(original)\n",
    "    count = 1\n",
    "    for index, char in enumerate(original_list):\n",
    "        if char == \" \":\n",
    "            current_list[index - count] += \" \"\n",
    "            count += 1\n",
    "    current = \"\".join(current_list)\n",
    "    return current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"早在23岁，伍兹就参与了世界上首个核反应堆Chicago Pile-1的建设，她是导师费米领导的项目团队中最年轻的一员。此外，伍兹在建立和使用实验所需的盖革计数器上起到关键作用。反应堆成功运转并达到自持状态时，她也是唯一在场的女性。曼哈顿计划中，她与费米合作；同时，她曾与第一任丈夫约翰·马歇尔（John Marshall）一同解决了汉福德区钚生产厂氙中毒的问题，并负责监督钚生产反应炉的建造和运行。\"\n",
    "a = translate(sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
