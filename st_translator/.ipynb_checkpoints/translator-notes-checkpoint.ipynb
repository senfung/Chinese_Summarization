{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from functools import reduce\n",
    "import mafan\n",
    "from mafan import text\n",
    "import itertools\n",
    "bos = \" <bos> \"\n",
    "eos = \" <eos> \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer Functions\n",
    "\n",
    "## Sentence Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zng(paragraph):\n",
    "    for sent in re.findall(u'[^!?。\\.\\!\\?]+[!?。\\.\\!\\?]?', paragraph, flags=re.U):\n",
    "        yield sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplified Chinese Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code for simplified to traditional mapping dictionary.\n",
    "\n",
    "We have a large dictionary *conversions.txt* that includes words, characters, common phrases, locations and idioms. Each entry contains the traditional chinese word and simplified chinese word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"conversions.txt\", \"r+\", encoding=\"utf-8\")\n",
    "\n",
    "s2t_dict = dict()\n",
    "\n",
    "for line in infile:\n",
    "    line = line.rstrip()\n",
    "    arr = line.split()\n",
    "    trad = arr[0]\n",
    "    sim = arr[1]\n",
    "    if sim not in s2t_dict:\n",
    "        s2t_dict[sim] = [trad]\n",
    "    else:\n",
    "        s2t_dict[sim].append(trad)\n",
    "s2t_dict['-'] = ['-']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokeniser is used for identifying dictionary words and phrases in the input sentence. We always prefer longer phrases because it gives more meaning and less translation mappings. Hence we use Byte Pair Encoding (BPE) for identifying words, while BPE candidates are constrained by the defined list of vocabs in the dictionary. Since the longest phrase in the dictionary has 8 characters we start with 8-character phrases and do it backwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(sentence, n = 8):\n",
    "    '''\n",
    "    This function tokenizes input sentences according to the dicitionary.\n",
    "    Input: a sentence or paragraph\n",
    "    Output: a list of tokens from the input in order according to the original paragraph; a list of non-chinese characters from the original text.\n",
    "    '''\n",
    "    text, charList = prepare(sentence)\n",
    "    token_list = []\n",
    "    input_text = text\n",
    "    for k in range(n, 0, -1):\n",
    "        candidates = [input_text[i:i + k] for i in range(len(input_text) - k + 1)]\n",
    "        for candidate in candidates:\n",
    "            if candidate in s2t_dict:\n",
    "                token_list.append(candidate)\n",
    "                input_text = re.sub(candidate, '', input_text)\n",
    "    final = sequencer(token_list, text)\n",
    "    return final, charList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_list(sentence_list, char_list):\n",
    "    count = 0\n",
    "    original = [] # sentence we want to output\n",
    "    \n",
    "    for word in sentence_list:\n",
    "        if \"-\" in word:\n",
    "            original.append(list(char_list[count]))\n",
    "            count += 1\n",
    "        else:\n",
    "            original.append(word)\n",
    "    return original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output(sentence, char_list):\n",
    "    count = 0\n",
    "    original = \"\" # sentence we want to output\n",
    "\n",
    "    for char in list(sentence):\n",
    "        if char == \"-\":\n",
    "            original += char_list[count] # append character if non-chinese\n",
    "            count += 1\n",
    "        else:\n",
    "            original += char # append chinese\n",
    "    return original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(sentence):\n",
    "    new = \"\" # input to your tokenizer\n",
    "    char_list = [] # punct / english to be omitted\n",
    "\n",
    "    for char in list(sentence):\n",
    "        if text.identify(char) is mafan.NEITHER:\n",
    "            new += \"-\" # sub - with non-chinese chars\n",
    "            char_list.append(char)\n",
    "        else:\n",
    "            new += char\n",
    "\n",
    "    return new, char_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequencer(tokens, example):\n",
    "\n",
    "    flags = [1] * len(example)\n",
    "    sequence = []\n",
    "    for token in tokens:\n",
    "        for match in re.finditer(token, example):\n",
    "            location = (token, match.span()[0], match.span()[1])\n",
    "            valid = reduce(lambda x,y:x*y, flags[location[1]:location[2]])\n",
    "            if valid:\n",
    "                sequence.append(location)\n",
    "                for i in range(location[1], location[2]):\n",
    "                    flags[i] = 0\n",
    "            else:\n",
    "                continue\n",
    "    sequence.sort(key=lambda x: x[1])\n",
    "    result = [x[0] for x in sequence]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to prepare our corpus.\n",
    "1. We will add paddings (sentinels) to our sentences.\n",
    "2. Take one sentence at a time.\n",
    "3. Change non-chinese words to FW to avoid data explosion.\n",
    "4. Slice the n-grams and add them to dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_stuff(order):\n",
    "    '''\n",
    "    This function divides the corpus into n-grams and stores them in dictionary.\n",
    "    Input: order of n-gram (like 2 for bi-gram)\n",
    "    Output: none\n",
    "    '''\n",
    "    infile = open(\"hk-zh.txt\", \"r+\") # this contains our corpus\n",
    "    start_padding = bos * order # add padding\n",
    "    end_padding = eos * order\n",
    "\n",
    "    for line in tqdm(infile, total=1314726):\n",
    "        line = line.rstrip()\n",
    "        sentences = list(zng(line)) # tokenize sentence by sentence\n",
    "        for sentence in sentences:\n",
    "            candidate = start_padding + sentence + end_padding # form sentence\n",
    "            word_list = candidate.split()\n",
    "            word_list_tokens = []\n",
    "            for word in word_list:\n",
    "                if not(bool(re.match('^[a-zA-Z0-9]+$', word))):\n",
    "                    word_list_tokens.append(word) # add if not chinese\n",
    "                else:\n",
    "                    word_list_tokens.append(\"FW\") # turn non-chinese (except punc) to FW\n",
    "            word_list = word_list_tokens\n",
    "            ordered = [word_list[i:i + order] for i in range(1, len(word_list) - order)] # extract n-grams through slicing\n",
    "            # for each ngram, convert to tuple and add to dictionary\n",
    "            for ngram in ordered:\n",
    "                ngram = tuple(ngram)\n",
    "                if ngram not in corpus:\n",
    "                    corpus[ngram] = 1\n",
    "                else:\n",
    "                    corpus[ngram] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say you want to extract till trigrams.\n",
    "\n",
    "We want to do 3 iterations, for trigram, bi-gram and then unigram. Each iteration takes 2 minutes. This is only time-consuming part of this code. Once you prep the dictionary, you don't need to do this again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = dict()\n",
    "# start_order = 2\n",
    "# for i in range(start_order, 0, -1):\n",
    "#     add_stuff(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you made the dictionary, dump it into a pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('corpus.pkl', 'wb') as handle:\n",
    "#     pickle.dump(corpus, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a way to load a pickle so you don't need to process data everytime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('corpus.pkl', 'rb') as fp:\n",
    "    corpus = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Candidate Lists\n",
    "\n",
    "1. Tokenize the input.\n",
    "2. Check the mappings of each input.\n",
    "3. Add all possible mappings to candidate list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(sentence):\n",
    "    '''\n",
    "    Returns list of possible mappings.\n",
    "    Input: Simplified chinese sentence\n",
    "    Output: List of lists. Each list has a set of possible traditional chinese tokens\n",
    "    '''\n",
    "    tokens, char_list = tokenizer(sentence)\n",
    "    candidate_list = []\n",
    "    for token in tokens:\n",
    "        candidate_list.append(s2t_dict[token])\n",
    "    candidate_list = output_list(candidate_list, char_list)\n",
    "    return(candidate_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum log-likelihood calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = 4526000 # total number of tokens in corpus\n",
    "\n",
    "def prob(word_list):\n",
    "    '''\n",
    "    Computes the log likelihood probability.\n",
    "    Input: A sequence of words in form of list\n",
    "    Output: Log probabilties\n",
    "    '''\n",
    "    word_list = tuple(word_list) # change word list to tuple\n",
    "    if word_list in corpus:\n",
    "        # word found in dictionary\n",
    "        numerator = corpus[word_list] # get the frequency of that word list\n",
    "        denominator = num_tokens # let denominator be num tokens\n",
    "        # cutoff the last word and check whether it's in corpus\n",
    "        if len(word_list[:-1]) > 1 and word_list[:-1] in corpus:\n",
    "            denom_list = word_list[:-1]\n",
    "            denominator = corpus[denom_list]\n",
    "        return log(numerator / denominator) # log of prob\n",
    "    else:\n",
    "        word_list = list(word_list) # convert it back to list\n",
    "        k = len(word_list) - 1 # backoff, reduce n gram length\n",
    "        if k > 0:\n",
    "            # recursive function, divide the sequence into smaller n and find probs\n",
    "            probs = [prob(word_list[i:i + k]) for i in range(len(word_list) - k + 1)]\n",
    "            return sum(probs)\n",
    "        else:\n",
    "            # we found an unseen word\n",
    "            if not(bool(re.match('^[a-zA-Z0-9]+$', word_list[0]))):\n",
    "                return log(1 / num_tokens) # return a small probability\n",
    "            else:\n",
    "                return prob([\"FW\"]) # we encountered a non-chinese word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backoff Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "def backoff(sentence, order):\n",
    "    '''\n",
    "    Calcuates log likelihood using backoff language model\n",
    "    Input: Sentence and order of the n-gram\n",
    "    Output: Log prob of that sentence\n",
    "    '''\n",
    "    score = 0\n",
    "    sentences = list(zng(sentence)) # sentence tokenizer\n",
    "    for sentence in sentences:\n",
    "        start_padding = bos * order # beginning padding\n",
    "        end_padding = eos * order # ending padding\n",
    "        candidate = start_padding + sentence + end_padding # add paddings\n",
    "        word_list = candidate.split()\n",
    "        word_list_tokens = []\n",
    "        for word in word_list:\n",
    "            # append only non-chinese words\n",
    "            if not(bool(re.match('^[a-zA-Z0-9]+$', word))):\n",
    "                word_list_tokens.append(word)\n",
    "            else:\n",
    "                word_list_tokens.append(\"FW\")\n",
    "        word_list = word_list_tokens\n",
    "        ordered = [word_list[i:i + order] for i in range(1, len(word_list) - order)] # shingle into n-grams\n",
    "        probs = [prob(x) for x in ordered] # calculate probabilities\n",
    "        score += sum(probs) # final answer\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    '''\n",
    "    Translate a given sentence to traditional\n",
    "    Input: Simplified Sentence\n",
    "    Output: Traditional Sentence\n",
    "    '''\n",
    "    candidates = convert(sentence) # get the candidate lists\n",
    "    final_sent = \"\"\n",
    "    for words in candidates:\n",
    "        if len(words) > 1:\n",
    "            # many to one mappings\n",
    "            score = -50000.0 # start with extreme negative value\n",
    "            likely = \"\"\n",
    "            for candidate in words:\n",
    "                temp = final_sent\n",
    "                temp = temp + \" \"  + candidate # add a candidate to temp sentence\n",
    "                current_score = backoff(temp, 2) # check perplexity\n",
    "                if current_score > score:\n",
    "                    # if performing good, include that\n",
    "                    score = current_score\n",
    "                    likely = candidate\n",
    "            final_sent = final_sent + \" \" + likely\n",
    "        else:\n",
    "            final_sent = final_sent + \" \" + words[0]\n",
    "    final_sent = final_sent.replace(\" \", \"\")\n",
    "    final_sent = add_back_spaces(sentence, final_sent)\n",
    "    return final_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_back_spaces(original, current):\n",
    "    current_list = list(current)\n",
    "    original_list = list(original)\n",
    "    count = 1\n",
    "    for index, char in enumerate(original_list):\n",
    "        if char == \" \":\n",
    "            current_list[index - count] += \" \"\n",
    "            count += 1\n",
    "    current = \"\".join(current_list)\n",
    "    return current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'姚松炎、周庭勢被「DQ」? 泛民質疑，政府再取消參選人資格涉政治篩選，要求律政司司長鄭若驊解釋法律理據。 有報道指，據全國人大常委會就《基本法》第一百零四條進行的釋法，代表泛民參選立法會港島及九龍西補選的香港眾志周庭和被「DQ」前議員姚松炎，勢被取消參選資格。律政司表示，法律政策專員黃惠沖將於稍後時間與泛民議員會面，確實時間待定。 民主派議員前晚在律政中心外靜坐要求與律政司司長鄭若驊會面不果後，昨在立法會召開記者招待會，要求鄭就撤銷參選人資格的理據，及其給予選舉主任的法律意見作出詳細交代。公民黨議員郭榮鏗批評，鄭不向公眾交代的做法是「冇承擔，冇責任」的表現，不能只把責任交托予公務員。 人民力量主席陳志全說，如參選設政治篩選是「假民主」的表現，形容事件「將令香港的民主制度倒退二十年」。民主黨主席胡志偉亦擔心，事件將令香港步向「一國一制」。 公共專業聯盟議員莫乃光提到，泛民將發起「一人一信」行動，向政府表達「反對DQ」的聲音。泛民明日將在公民廣場舉行集會，要求政府立即核實姚、週二人的參選資格。 姚松炎則表示，如政府引用《基本法》第一百零四條的釋法取消自己的參選資格，理據並不充分。他認為，釋法只是指出自己的宣誓無效，不會被重新安排宣誓，但無說明自己不能在同一屆立法會其他界別的議席宣誓，亦未提及自己的參選權會被剝奪。他反問不能重新宣誓是否「一生都不能宣誓」，現有如「剝奪自己政治權利終身」。 行政會議成員湯家驊在電臺節目上說，政府如援以《基本法》第一百零四條的釋法及去年取消議員法律資格的法律決定，並不能作為撤銷姚松炎參選資格的法律理據。他解釋，上次的釋法及法律決定是按照「成為議員的就職資格」而作出，「就職資格」與「參選資格」不相同，違反就職資格不能被引申為不可參選。 對於周庭所屬的香港眾志主張民主自決，並將「港獨」列為選項，湯家驊承認，倘若自決是《基本法》框架外的主張，她被取消資格的機會的確會「多少少」。但他指出，政府需仔細研究香港眾志的黨綱，「不能單靠黨綱文字，便裁定他們不擁護《基本法》」。 公民黨黨魁楊岳橋指，釋法內容並非剝奪政治權利，倘政府以釋法為理據，限制姚松炎出選，做法是「移船就磡」，更反問「是否有人誠心跪玻璃悔改，都不能透過補選再次入閘？」他期望鄭若驊儘快交代，政府不要再利用公務員或選舉主任作政治決定。 前學聯常委司徒子朗則在其facebook貼文，希望資助五萬元，尋找有志之士參加九龍西補選，擔任「真PLAN B」，因為他認為將替代姚松炎出選的民主黨袁海文的支持度不足。 民建聯鄭泳舜及報稱獨立的前青年民建聯成員、物理治療師蔡東洲亦報名參選九龍西。港島區的參選人還有新民黨陳家及任亮憲。'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"姚松炎、周庭势被「DQ」? 泛民质疑，政府再取消参选人资格涉政治筛选，要求律政司司长郑若骅解释法律理据。 有报道指，据全国人大常委会就《基本法》第一百零四条进行的释法，代表泛民参选立法会港岛及九龙西补选的香港众志周庭和被「DQ」前议员姚松炎，势被取消参选资格。律政司表示，法律政策专员黄惠冲将于稍后时间与泛民议员会面，确实时间待定。 民主派议员前晚在律政中心外静坐要求与律政司司长郑若骅会面不果后，昨在立法会召开记者招待会，要求郑就撤销参选人资格的理据，及其给予选举主任的法律意见作出详细交代。公民党议员郭荣铿批评，郑不向公众交代的做法是「冇承担，冇责任」的表现，不能只把责任交托予公务员。 人民力量主席陈志全说，如参选设政治筛选是「假民主」的表现，形容事件「将令香港的民主制度倒退二十年」。民主党主席胡志伟亦担心，事件将令香港步向「一国一制」。 公共专业联盟议员莫乃光提到，泛民将发起「一人一信」行动，向政府表达「反对DQ」的声音。泛民明日将在公民广场举行集会，要求政府立即核实姚、周二人的参选资格。 姚松炎则表示，如政府引用《基本法》第一百零四条的释法取消自己的参选资格，理据并不充分。他认为，释法只是指出自己的宣誓无效，不会被重新安排宣誓，但无说明自己不能在同一届立法会其他界别的议席宣誓，亦未提及自己的参选权会被剥夺。他反问不能重新宣誓是否「一生都不能宣誓」，现有如「剥夺自己政治权利终身」。 行政会议成员汤家骅在电台节目上说，政府如援以《基本法》第一百零四条的释法及去年取消议员法律资格的法律决定，并不能作为撤销姚松炎参选资格的法律理据。他解释，上次的释法及法律决定是按照「成为议员的就职资格」而作出，「就职资格」与「参选资格」不相同，违反就职资格不能被引申为不可参选。 对于周庭所属的香港众志主张民主自决，并将「港独」列为选项，汤家骅承认，倘若自决是《基本法》框架外的主张，她被取消资格的机会的确会「多少少」。但他指出，政府需仔细研究香港众志的党纲，「不能单靠党纲文字，便裁定他们不拥护《基本法》」。 公民党党魁杨岳桥指，释法内容并非剥夺政治权利，倘政府以释法为理据，限制姚松炎出选，做法是「移船就磡」，更反问「是否有人诚心跪玻璃悔改，都不能透过补选再次入闸？」他期望郑若骅尽快交代，政府不要再利用公务员或选举主任作政治决定。 前学联常委司徒子朗则在其facebook贴文，希望资助五万元，寻找有志之士参加九龙西补选，担任「真PLAN B」，因为他认为将替代姚松炎出选的民主党袁海文的支持度不足。 民建联郑泳舜及报称独立的前青年民建联成员、物理治疗师蔡东洲亦报名参选九龙西。港岛区的参选人还有新民党陈家珮及任亮宪。\"\n",
    "a = translate(sentence)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
