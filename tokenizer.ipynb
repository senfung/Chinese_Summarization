{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import mafan\n",
    "from mafan import text\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"./conversions.txt\", \"r+\")\n",
    "s2t_dict = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in infile:\n",
    "    line = line.rstrip()\n",
    "    arr = line.split()\n",
    "    trad = arr[0]\n",
    "    sim = arr[1]\n",
    "    if sim not in s2t_dict:\n",
    "        s2t_dict[sim] = [trad]\n",
    "    else:\n",
    "        s2t_dict[sim].append(trad)\n",
    "s2t_dict['-'] = ['-']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(sentence):\n",
    "    new = \"\"  # input to your tokenizer\n",
    "    char_list = []  # punct / english to be omitted\n",
    "\n",
    "    for char in list(sentence):\n",
    "        if text.identify(char) is mafan.NEITHER:\n",
    "            new += \"-\"  # sub - with non-chinese chars\n",
    "            char_list.append(char)\n",
    "            continue\n",
    "        else:\n",
    "            new += char\n",
    "\n",
    "    return new, char_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequencer(tokens, example):\n",
    "\n",
    "    flags = [1] * len(example)\n",
    "    sequence = []\n",
    "    tokens.sort(key=lambda x: -len(x))\n",
    "    if \".\" in tokens:\n",
    "        dotCount = tokens.count(\".\")\n",
    "        for d in range(dotCount):\n",
    "            tokens.remove(\".\")\n",
    "            tokens.append(\".\")\n",
    "\n",
    "    for token in tokens:\n",
    "        # print(\"Token \" + token)\n",
    "        for match in re.finditer(token, example):\n",
    "            location = (token, match.span()[0], match.span()[1])\n",
    "            valid = reduce(lambda x, y: x*y, flags[location[1]:location[2]])\n",
    "            if valid:\n",
    "                sequence.append(location)\n",
    "                for i in range(location[1], location[2]):\n",
    "                    flags[i] = 0\n",
    "            else:\n",
    "                continue\n",
    "    sequence.sort(key=lambda x: x[1])\n",
    "    result = [x[0] for x in sequence]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(sentence, original, n=8):\n",
    "    text, charList = prepare(sentence)\n",
    "    token_list = []\n",
    "    input_text = text\n",
    "    for k in range(n, 0, -1):\n",
    "        candidates = [input_text[i:i + k]\n",
    "                      for i in range(len(input_text) - k + 1)]\n",
    "\n",
    "        for candidate in candidates:\n",
    "            if '-' in candidate:\n",
    "                continue\n",
    "            elif candidate in s2t_dict:\n",
    "                token_list.append(candidate)\n",
    "                input_text = re.sub(candidate, '', input_text)\n",
    "    token_set = list(set(token_list))\n",
    "\n",
    "    sentence_parts = sentence.split()\n",
    "    ordered_parts = sequencer(sentence_parts, original)\n",
    "\n",
    "    string = \" \".join(x for x in ordered_parts)\n",
    "\n",
    "    final = sequencer(token_set+charList, string)\n",
    "\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "香港大学 为 亚洲 之 国际 大学 ， 借 国际化 、 创新 性 及 跨 范畴 发挥 其 影响力 ； 凭 卓越 研究 、 优秀 教学 、 知识 与 技术 之 流转 ， 吸引 及 培育 全球 英 才 ； 并 透过 参与 环球 事务 、 其 地区 影响力 及 与 国内 之 紧密 连系 ， 为 促进 社会 进步 作出 贡献 。\n"
     ]
    }
   ],
   "source": [
    "input = \"香港大学为亚洲之国际大学，借国际化、创新性及跨范畴发挥其影响力；凭卓越研究、优秀教学、知识与技术之交流转移，吸引及培育全球英才；并透过参与环球事务、其地区影响力及与中国内地之紧密连系，为促进社会进步作出贡献。\"\n",
    "tokens = tokenizer(input, input)\n",
    "output = \" \".join(x for x in tokens)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一 辆 小 轿车 ， 一 名 女 司机 ， 竟 造成 9 死 2 4 伤 。 日前 ， 深圳市 交警 局 对 事故 进行 通报 ： 从 目前 证据 看 ， 事故 系 司机 超速 行驶 且 操作 不当 导致 。 目前 2 4 名 伤员 已 有 6 名 治愈 出院 ， 其余 正 接受 治疗 ， 预计 事故 赔偿 费 或 超 一 千万 元 。\n"
     ]
    }
   ],
   "source": [
    "input = \"一辆小轿车，一名女司机，竟造成9死24伤。日前，深圳市交警局对事故进行通报：从目前证据看，事故系司机超速行驶且操作不当导致。目前24名伤员已有6名治愈出院，其余正接受治疗，预计事故赔偿费或超一千万元。\"\n",
    "tokens = tokenizer(input, input)\n",
    "output = \" \".join(x for x in tokens)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jieba output   \n",
    "一辆 小轿车 ， 一名 女司机 ， 竟 造成 9 死 24 伤 。 日前 ， 深圳市 交警 局对 事故 进行 通报 ： 从 目前 证据 看 ， 事故 系 司机 超速行驶 且 操作 不当 导致 。 目前 24 名 伤员 已有 6 名 治愈 出院 ， 其余 正 接受 治疗 ， 预计 事故 赔偿费 或超 一千万元 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
